{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-09 13:17:16.480467: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-09 13:17:21.942103: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1636] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 78791 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:b7:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.chdir(\"/sfs/qumulo/qhome/jtb3sud/PARCv2\")\n",
    "import PARC.data.data_meta_learning as data\n",
    "#from parc import misc, metrics, visualization\n",
    "from PARC.model import model_meta_learning as model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 183118.npy\n",
      "Processing 183135.npy\n",
      "Processing 183127.npy\n",
      "Processing 183134.npy\n",
      "Processing 183115.npy\n",
      "Processing 183132.npy\n",
      "Processing 183114.npy\n",
      "Processing 183123.npy\n",
      "Processing 183140.npy\n",
      "Processing 183130.npy\n",
      "Processing 183133.npy\n",
      "Processing 183129.npy\n",
      "Processing 183111.npy\n",
      "Processing 183105.npy\n",
      "Processing 183102.npy\n",
      "Processing 183113.npy\n",
      "Processing 183137.npy\n",
      "Processing 183136.npy\n",
      "Processing 183119.npy\n",
      "Processing 183144.npy\n",
      "Processing 183110.npy\n",
      "Processing 183112.npy\n",
      "Processing 183122.npy\n",
      "Processing 183104.npy\n",
      "Processing 183128.npy\n",
      "Processing 183141.npy\n",
      "Processing 183145.npy\n",
      "Processing 183103.npy\n",
      "Processing 183107.npy\n",
      "Processing 183106.npy\n",
      "Processing 183121.npy\n",
      "Processing 183108.npy\n",
      "Processing 183142.npy\n",
      "Processing 183109.npy\n",
      "Processing 183125.npy\n"
     ]
    }
   ],
   "source": [
    "# Get data and normalization\n",
    "state_seq_whole, vel_seq_whole = data.clip_raw_data(sequence_length = 52, n_state_var = 3, purpose = \"diff_training\", folder_path = '/scratch/jtb3sud/meta_data/tnt/training', image_size = (128, 208))\n",
    "state_seq_norm = data.data_normalization(state_seq_whole,3)\n",
    "vel_seq_norm = data.data_normalization(vel_seq_whole,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 1: Differentiator training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tf.dataset\n",
    "dataset_input = tf.data.Dataset.from_tensor_slices((state_seq_norm[0][:,:,:,:3],vel_seq_norm[0][:,:,:,:2]))\n",
    "dataset_label = tf.data.Dataset.from_tensor_slices((state_seq_norm[0][:,:,:,-3:],vel_seq_norm[0][:,:,:,-2:]))\n",
    "dataset = tf.data.Dataset.zip((dataset_input, dataset_label))\n",
    "dataset = dataset.shuffle(buffer_size = 2192) \n",
    "dataset = dataset.batch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(339, 128, 208, 156) (339, 128, 208, 104)\n"
     ]
    }
   ],
   "source": [
    "print(state_seq_whole.shape, vel_seq_whole.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(339, 128, 208, 156) (339, 128, 208, 104)\n"
     ]
    }
   ],
   "source": [
    "print(state_seq_norm[0].shape, vel_seq_norm[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_BatchDataset element_spec=((TensorSpec(shape=(None, 128, 208, 3), dtype=tf.float64, name=None), TensorSpec(shape=(None, 128, 208, 2), dtype=tf.float64, name=None)), (TensorSpec(shape=(None, 128, 208, 3), dtype=tf.float64, name=None), TensorSpec(shape=(None, 128, 208, 2), dtype=tf.float64, name=None)))>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-09 13:19:32.258026: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8904\n",
      "2024-05-09 13:19:40.685727: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f78d04126c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-05-09 13:19:40.685789: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100-SXM4-80GB, Compute Capability 8.0\n",
      "2024-05-09 13:19:40.798842: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-05-09 13:19:41.656383: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 97s 979ms/step - total_loss: 18035.5371\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f9401307490>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "# step size needs to be changed to 1/total number of steps\n",
    "#parc = model.PARCv2(n_state_var = 3, n_time_step = 1, step_size= 1/60, solver = \"rk4\", mode = \"differentiator_training\")\n",
    "parc = model.PARCv2(n_state_var = 3, n_time_step = 1, step_size= 1/52, solver = \"rk4\", mode = \"differentiator_training\", image_size = (128, 208))\n",
    "parc.differentiator.load_weights('/sfs/qumulo/qhome/jtb3sud/PARCv2/Pretrained_Weights/em/parc2_diff_rk4.h5')\n",
    "parc.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.00001, beta_1 = 0.9, beta_2 = 0.999))\n",
    "parc.fit(dataset, epochs = 1, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "parc.differentiator.save_weights('/sfs/qumulo/qhome/jtb3sud/PARCv2/Pretrained_Weights/tnt/parc2_diff_rk4.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2: Data-driven integration training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 57s 496ms/step - total_loss: 17915.3359\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f93e6402740>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pretrain integrator\n",
    "tf.keras.backend.clear_session()\n",
    "parc = model.PARCv2(n_state_var = 3, n_time_step = 1, step_size= 1/52, solver = \"rk4\", mode = \"integrator_training\", image_size = (128, 208))\n",
    "parc.integrator.load_weights('/sfs/qumulo/qhome/jtb3sud/PARCv2/Pretrained_Weights/em/parc2_int_rk4.h5')\n",
    "parc.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.00001, beta_1 = 0.9, beta_2 = 0.999))\n",
    "parc.fit(dataset, epochs = 1, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "parc.integrator.save_weights('/sfs/qumulo/qhome/jtb3sud/PARCv2/Pretrained_Weights/tnt/parc2_int_rk4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow 2.13.0",
   "language": "python",
   "name": "tensorflow-2.13.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
