{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-08 18:18:12.565941: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-08 18:18:19.588005: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1636] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1284 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.chdir(\"/sfs/qumulo/qhome/jtb3sud/PARCv2\")\n",
    "import PARC.data.data_meta_learning as data\n",
    "#from parc import misc, metrics, visualization\n",
    "from PARC.model import model_meta_learning as model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 183118.npy\n",
      "Processing 183135.npy\n",
      "Processing 183127.npy\n",
      "Processing 183134.npy\n",
      "Processing 183115.npy\n",
      "Processing 183132.npy\n",
      "Processing 183114.npy\n",
      "Processing 183123.npy\n",
      "Processing 183140.npy\n",
      "Processing 183130.npy\n",
      "Processing 183133.npy\n",
      "Processing 183129.npy\n",
      "Processing 183111.npy\n",
      "Processing 183105.npy\n",
      "Processing 183102.npy\n",
      "Processing 183113.npy\n",
      "Processing 183137.npy\n",
      "Processing 183136.npy\n",
      "Processing 183119.npy\n",
      "Processing 183144.npy\n",
      "Processing 183110.npy\n",
      "Processing 183112.npy\n",
      "Processing 183122.npy\n",
      "Processing 183104.npy\n",
      "Processing 183128.npy\n",
      "Processing 183141.npy\n",
      "Processing 183145.npy\n",
      "Processing 183103.npy\n",
      "Processing 183107.npy\n",
      "Processing 183106.npy\n",
      "Processing 183121.npy\n",
      "Processing 183108.npy\n",
      "Processing 183142.npy\n",
      "Processing 183109.npy\n",
      "Processing 183125.npy\n"
     ]
    }
   ],
   "source": [
    "# Get data and normalization\n",
    "state_seq_whole, vel_seq_whole = data.clip_raw_data(sequence_length = 52, n_state_var = 3, purpose = \"diff_training\", folder_path = '/scratch/jtb3sud/meta_data/tnt/training', image_size = (128, 208))\n",
    "state_seq_norm = data.data_normalization(state_seq_whole,3)\n",
    "vel_seq_norm = data.data_normalization(vel_seq_whole,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 1: Differentiator training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tf.dataset\n",
    "dataset_input = tf.data.Dataset.from_tensor_slices((state_seq_norm[0][:,:,:,:3],vel_seq_norm[0][:,:,:,:2]))\n",
    "dataset_label = tf.data.Dataset.from_tensor_slices((state_seq_norm[0][:,:,:,-3:],vel_seq_norm[0][:,:,:,-2:]))\n",
    "dataset = tf.data.Dataset.zip((dataset_input, dataset_label))\n",
    "dataset = dataset.shuffle(buffer_size = 2192) \n",
    "dataset = dataset.batch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(339, 128, 208, 156) (339, 128, 208, 104)\n"
     ]
    }
   ],
   "source": [
    "print(state_seq_whole.shape, vel_seq_whole.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(339, 128, 208, 156) (339, 128, 208, 104)\n"
     ]
    }
   ],
   "source": [
    "print(state_seq_norm[0].shape, vel_seq_norm[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_BatchDataset element_spec=((TensorSpec(shape=(None, 128, 208, 3), dtype=tf.float64, name=None), TensorSpec(shape=(None, 128, 208, 2), dtype=tf.float64, name=None)), (TensorSpec(shape=(None, 128, 208, 3), dtype=tf.float64, name=None), TensorSpec(shape=(None, 128, 208, 2), dtype=tf.float64, name=None)))>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"/sfs/qumulo/qhome/jtb3sud/PARCv2/PARC/model/model_meta_learning.py\", line 156, in train_step  *\n        total_loss  = (tf.keras.losses.MeanAbsoluteError(reduction = 'sum')(state_pred,state_var_gt) +\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 142, in __call__  **\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 1749, in mean_absolute_error\n        return backend.mean(tf.abs(y_pred - y_true), axis=-1)\n\n    ValueError: Dimensions must be equal, but are 3 and 156 for '{{node mean_absolute_error/sub}} = Sub[T=DT_FLOAT](Cast_2, concatenate_1/concat)' with input shapes: [?,128,208,3], [?,128,208,156].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m parc\u001b[38;5;241m.\u001b[39mdifferentiator\u001b[38;5;241m.\u001b[39mload_weights(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/sfs/qumulo/qhome/jtb3sud/PARCv2/Pretrained_Weights/em/parc2_diff_rk4.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m parc\u001b[38;5;241m.\u001b[39mcompile(optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.00001\u001b[39m, beta_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.9\u001b[39m, beta_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.999\u001b[39m))\n\u001b[0;32m----> 7\u001b[0m \u001b[43mparc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file_klxvldp.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filevracu2l6.py:77\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     75\u001b[0m     state_pred \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(Concatenate), (), \u001b[38;5;28mdict\u001b[39m(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), fscope), (ag__\u001b[38;5;241m.\u001b[39mld(state_whole),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     76\u001b[0m     vel_pred \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(Concatenate), (), \u001b[38;5;28mdict\u001b[39m(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), fscope), (ag__\u001b[38;5;241m.\u001b[39mld(vel_whole),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m---> 77\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m (\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlosses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMeanAbsoluteError\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_pred\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_var_gt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mMeanAbsoluteError, (), \u001b[38;5;28mdict\u001b[39m(reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m), fscope), (ag__\u001b[38;5;241m.\u001b[39mld(vel_pred), ag__\u001b[38;5;241m.\u001b[39mld(velocity_gt)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     78\u001b[0m grads \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tape)\u001b[38;5;241m.\u001b[39mgradient, (ag__\u001b[38;5;241m.\u001b[39mld(total_loss), ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mtrainable_weights), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     79\u001b[0m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mapply_gradients, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mzip\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mld(grads), ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mtrainable_weights), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"/sfs/qumulo/qhome/jtb3sud/PARCv2/PARC/model/model_meta_learning.py\", line 156, in train_step  *\n        total_loss  = (tf.keras.losses.MeanAbsoluteError(reduction = 'sum')(state_pred,state_var_gt) +\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 142, in __call__  **\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 1749, in mean_absolute_error\n        return backend.mean(tf.abs(y_pred - y_true), axis=-1)\n\n    ValueError: Dimensions must be equal, but are 3 and 156 for '{{node mean_absolute_error/sub}} = Sub[T=DT_FLOAT](Cast_2, concatenate_1/concat)' with input shapes: [?,128,208,3], [?,128,208,156].\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "# step size needs to be changed to 1/total number of steps\n",
    "#parc = model.PARCv2(n_state_var = 3, n_time_step = 1, step_size= 1/60, solver = \"rk4\", mode = \"differentiator_training\")\n",
    "parc = model.PARCv2(n_state_var = 3, n_time_step = 52, step_size= 1/52, solver = \"rk4\", mode = \"differentiator_training\", image_size = (128, 208))\n",
    "parc.differentiator.load_weights('/sfs/qumulo/qhome/jtb3sud/PARCv2/Pretrained_Weights/em/parc2_diff_rk4.h5')\n",
    "parc.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.00001, beta_1 = 0.9, beta_2 = 0.999))\n",
    "parc.fit(dataset, epochs = 100, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parc.differentiator.save_weights('/sfs/qumulo/qhome/jtb3sud/PARCv2/Pretrained_Weights/tnt/parc2_diff_rk4.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2: Data-driven integration training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-30 11:52:17.496255: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8904\n",
      "2024-04-30 11:52:20.861035: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c617b17e00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-04-30 11:52:20.861077: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100-SXM4-80GB, Compute Capability 8.0\n",
      "2024-04-30 11:52:20.974633: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-04-30 11:52:21.937055: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232/232 [==============================] - 154s 498ms/step - total_loss: 686.6307\n",
      "Epoch 2/125\n",
      "232/232 [==============================] - 115s 494ms/step - total_loss: 567.7000\n",
      "Epoch 3/125\n",
      "232/232 [==============================] - 115s 494ms/step - total_loss: 545.1809\n",
      "Epoch 4/125\n",
      "232/232 [==============================] - 115s 494ms/step - total_loss: 523.2788\n",
      "Epoch 5/125\n",
      "232/232 [==============================] - 115s 494ms/step - total_loss: 514.4716\n",
      "Epoch 6/125\n",
      "232/232 [==============================] - 115s 494ms/step - total_loss: 504.3910\n",
      "Epoch 7/125\n",
      "232/232 [==============================] - 115s 494ms/step - total_loss: 494.5680\n",
      "Epoch 8/125\n",
      "232/232 [==============================] - 115s 494ms/step - total_loss: 488.8584\n",
      "Epoch 9/125\n",
      "232/232 [==============================] - 115s 494ms/step - total_loss: 486.2922\n",
      "Epoch 10/125\n",
      "232/232 [==============================] - 115s 494ms/step - total_loss: 484.7857\n",
      "Epoch 11/125\n",
      "232/232 [==============================] - 115s 494ms/step - total_loss: 482.5334\n",
      "Epoch 12/125\n",
      "232/232 [==============================] - 115s 494ms/step - total_loss: 483.2994\n",
      "Epoch 13/125\n",
      "232/232 [==============================] - 115s 494ms/step - total_loss: 479.1026\n",
      "Epoch 14/125\n",
      "232/232 [==============================] - 115s 494ms/step - total_loss: 470.9211\n",
      "Epoch 15/125\n",
      "232/232 [==============================] - 115s 494ms/step - total_loss: 476.1097\n",
      "Epoch 16/125\n",
      "232/232 [==============================] - 115s 494ms/step - total_loss: 474.2356\n",
      "Epoch 17/125\n",
      "232/232 [==============================] - 115s 494ms/step - total_loss: 474.5525\n",
      "Epoch 18/125\n",
      "232/232 [==============================] - 115s 494ms/step - total_loss: 488.5805\n",
      "Epoch 19/125\n",
      "232/232 [==============================] - 115s 494ms/step - total_loss: 465.3564\n",
      "Epoch 20/125\n",
      "232/232 [==============================] - 115s 494ms/step - total_loss: 478.5482\n",
      "Epoch 21/125\n",
      "232/232 [==============================] - 115s 494ms/step - total_loss: 469.6801\n",
      "Epoch 22/125\n",
      "232/232 [==============================] - 115s 494ms/step - total_loss: 468.0078\n",
      "Epoch 23/125\n",
      "232/232 [==============================] - 115s 494ms/step - total_loss: 464.9856\n",
      "Epoch 24/125\n",
      "232/232 [==============================] - 115s 494ms/step - total_loss: 463.4934\n",
      "Epoch 25/125\n",
      "232/232 [==============================] - 115s 494ms/step - total_loss: 461.5241\n",
      "Epoch 26/125\n",
      "232/232 [==============================] - 115s 494ms/step - total_loss: 460.8741\n",
      "Epoch 27/125\n",
      "232/232 [==============================] - 115s 494ms/step - total_loss: 458.2888\n",
      "Epoch 28/125\n",
      "232/232 [==============================] - 115s 494ms/step - total_loss: 461.5765\n",
      "Epoch 29/125\n",
      "121/232 [==============>...............] - ETA: 54s - total_loss: 448.7494"
     ]
    }
   ],
   "source": [
    "# Pretrain integrator\n",
    "tf.keras.backend.clear_session()\n",
    "parc = model.PARCv2(n_state_var = 3, n_time_step = 1, step_size= 1/60, solver = \"rk4\", mode = \"integrator_training\")\n",
    "parc.integrator.load_weights('/sfs/qumulo/qhome/jtb3sud/PARCv2/Pretrained_Weights/em/parc2_int_rk4.h5')\n",
    "parc.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.00001, beta_1 = 0.9, beta_2 = 0.999))\n",
    "parc.fit(dataset, epochs = 125, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parc.integrator.save_weights('/sfs/qumulo/qhome/jtb3sud/PARCv2/Pretrained_Weights/tnt/parc2_int_rk4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow 2.13.0",
   "language": "python",
   "name": "tensorflow-2.13.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
