{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-01 17:38:22.414293: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.chdir(\".\")\n",
    "from parc.data.data import EnergeticMatDataPipeLine as EmData\n",
    "#from parc import misc, metrics, visualization\n",
    "from parc.model import model_em as model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "void_2\n",
      "void_5\n",
      "void_7\n",
      "void_13\n",
      "void_14\n",
      "void_15\n",
      "void_16\n",
      "void_17\n",
      "void_18\n",
      "void_19\n",
      "void_20\n",
      "void_21\n",
      "void_22\n",
      "void_23\n",
      "void_26\n",
      "void_28\n",
      "void_29\n",
      "void_30\n",
      "void_31\n",
      "void_32\n",
      "void_33\n",
      "void_35\n",
      "void_36\n",
      "void_37\n",
      "void_38\n",
      "void_39\n",
      "void_40\n",
      "void_41\n",
      "void_42\n",
      "void_43\n",
      "void_46\n",
      "void_50\n",
      "void_51\n",
      "void_52\n",
      "void_53\n",
      "void_54\n",
      "void_55\n",
      "void_56\n",
      "void_57\n",
      "void_60\n",
      "void_61\n",
      "void_62\n",
      "void_63\n",
      "void_64\n",
      "void_65\n",
      "void_66\n",
      "void_67\n",
      "void_69\n",
      "void_70\n",
      "void_71\n",
      "void_72\n",
      "void_73\n",
      "void_74\n",
      "void_75\n",
      "void_81\n",
      "void_82\n",
      "void_87\n",
      "void_100\n",
      "void_101\n",
      "void_102\n",
      "void_103\n",
      "void_104\n",
      "void_105\n",
      "void_106\n",
      "void_107\n",
      "void_108\n",
      "void_109\n",
      "void_110\n",
      "void_111\n",
      "void_112\n",
      "void_113\n",
      "void_114\n",
      "void_115\n",
      "void_117\n",
      "void_118\n",
      "void_119\n",
      "void_120\n",
      "void_122\n",
      "void_123\n",
      "void_124\n",
      "void_125\n",
      "void_126\n",
      "void_127\n",
      "void_128\n",
      "void_129\n",
      "void_130\n",
      "void_131\n",
      "void_133\n",
      "void_134\n",
      "void_135\n",
      "void_136\n",
      "void_137\n",
      "void_138\n",
      "void_139\n",
      "void_140\n",
      "void_141\n",
      "void_142\n",
      "void_143\n",
      "void_146\n",
      "void_147\n"
     ]
    }
   ],
   "source": [
    "# Get data and normalization\n",
    "state_seq_whole, vel_seq_whole = EmData.clip_raw_data(idx_range = (0,150))\n",
    "state_seq_norm = EmData.data_normalization(state_seq_whole,3)\n",
    "vel_seq_norm = EmData.data_normalization(vel_seq_whole,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 1: Differentiator training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-01 17:39:23.206071: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1636] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46594 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:81:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Create tf.dataset\n",
    "dataset_input = tf.data.Dataset.from_tensor_slices((state_seq_norm[0][:,:,:,:3],vel_seq_norm[0][:,:,:,:2]))\n",
    "dataset_label = tf.data.Dataset.from_tensor_slices((state_seq_norm[0][:,:,:,-3:],vel_seq_norm[0][:,:,:,-2:]))\n",
    "dataset = tf.data.Dataset.zip((dataset_input, dataset_label))\n",
    "dataset = dataset.shuffle(buffer_size = 2192) \n",
    "dataset = dataset.batch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"model_5\" (type Functional).\n\nInput 0 of layer \"model\" is incompatible with the layer: expected shape=(None, 128, 192, 5), found shape=(None, 128, 256, 5)\n\nCall arguments received by layer \"model_5\" (type Functional):\n  • inputs=tf.Tensor(shape=(None, 128, 256, 5), dtype=float32)\n  • training=None\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39mclear_session()\n\u001b[0;32m----> 2\u001b[0m parc \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPARCv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_state_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_time_step\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolver\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrk\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiator_training\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# parc.differentiator.load_weights('parc2_diff_rk4.h5')\u001b[39;00m\n\u001b[1;32m      4\u001b[0m parc\u001b[38;5;241m.\u001b[39mcompile(optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.00001\u001b[39m, beta_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.9\u001b[39m, beta_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.999\u001b[39m))\n",
      "File \u001b[0;32m/sfs/weka/scratch/xc7ts/hypersonic_flow/PARCv2/parc/model/model_em.py:108\u001b[0m, in \u001b[0;36mPARCv2.__init__\u001b[0;34m(self, n_state_var, n_time_step, step_size, solver, mode, use_data_driven_int, differentiator_backbone, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layer1 \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mInput((\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layer2 \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mInput((\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_layer1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_layer2\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28msuper\u001b[39m(PARCv2, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    111\u001b[0m     inputs\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layer1, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layer2], outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    112\u001b[0m )\n",
      "File \u001b[0;32m/sfs/weka/scratch/xc7ts/hypersonic_flow/PARCv2/parc/model/model_em.py:135\u001b[0m, in \u001b[0;36mPARCv2.call\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    133\u001b[0m res \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_time_step):    \n\u001b[0;32m--> 135\u001b[0m     input_seq_current, update \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplicit_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_seq_current\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_data_driven_int \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m         state_var_next, velocity_next \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintegrator([update[:,:,:,:\u001b[38;5;241m3\u001b[39m],update[:,:,:,\u001b[38;5;241m3\u001b[39m:],input_seq_current[:,:,:,:\u001b[38;5;241m3\u001b[39m], input_seq_current[:,:,:,\u001b[38;5;241m3\u001b[39m:]])\n",
      "File \u001b[0;32m/sfs/weka/scratch/xc7ts/hypersonic_flow/PARCv2/parc/model/model_em.py:185\u001b[0m, in \u001b[0;36mPARCv2.explicit_update\u001b[0;34m(self, input_seq_current)\u001b[0m\n\u001b[1;32m    183\u001b[0m     input_seq_current, update \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheun_update(input_seq_current)\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 185\u001b[0m     input_seq_current, update \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meuler_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_seq_current\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m input_seq_current, update\n",
      "File \u001b[0;32m/sfs/weka/scratch/xc7ts/hypersonic_flow/PARCv2/parc/model/model_em.py:231\u001b[0m, in \u001b[0;36mPARCv2.euler_update\u001b[0;34m(self, input_seq_current)\u001b[0m\n\u001b[1;32m    229\u001b[0m input_seq_current \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mclip_by_value(input_seq_current, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# Compute update\u001b[39;00m\n\u001b[0;32m--> 231\u001b[0m update \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdifferentiator\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_seq_current\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m input_seq_current \u001b[38;5;241m=\u001b[39m input_seq_current \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_size\u001b[38;5;241m*\u001b[39mupdate \n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m input_seq_current, update\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py:298\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;241m!=\u001b[39m dim:\n\u001b[0;32m--> 298\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    299\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    300\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    301\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    302\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdisplay_shape(x\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    303\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"model_5\" (type Functional).\n\nInput 0 of layer \"model\" is incompatible with the layer: expected shape=(None, 128, 192, 5), found shape=(None, 128, 256, 5)\n\nCall arguments received by layer \"model_5\" (type Functional):\n  • inputs=tf.Tensor(shape=(None, 128, 256, 5), dtype=float32)\n  • training=None\n  • mask=None"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "parc = model.PARCv2(n_state_var = 3, n_time_step = 1, step_size= 1/15, solver = \"rk\", mode = \"differentiator_training\")\n",
    "# parc.differentiator.load_weights('parc2_diff_rk4.h5')\n",
    "parc.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.00001, beta_1 = 0.9, beta_2 = 0.999))\n",
    "parc.fit(dataset, epochs = 50, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parc.differentiator.save_weights('parc2_diff_rk4.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2: Data-driven integration training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pretrain integrator\n",
    "tf.keras.backend.clear_session()\n",
    "parc = model.PARCv2(n_state_var = 3, n_time_step = 1, step_size= 1/15, solver = \"euler\", mode = \"integrator_training\")\n",
    "parc.differentiator.load_weights('./parc2_diff_rk4.h5')\n",
    "parc.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.00001, beta_1 = 0.9, beta_2 = 0.999))\n",
    "parc.fit(dataset, epochs = 50, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parc.integrator.save_weights('parc2_int_rk4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import skimage\n",
    "from skimage.measure import block_reduce\n",
    "\n",
    "\n",
    "def clip_raw_data(idx_range, sequence_length=2, n_state_var=3, purpose = \"diff_training\"):\n",
    "    state_seq_whole = []\n",
    "    vel_seq_whole = []\n",
    "\n",
    "    for i in range(idx_range[0],idx_range[1]):\n",
    "        file_path = os.path.join(os.sep,'project','SDS','research', 'Nguyen_storage', 'data', 'single_void_data', f'void_{i}.npy')\n",
    "        if os.path.exists(file_path):\n",
    "            raw_data = np.float32(np.load(file_path))\n",
    "            data_shape = raw_data.shape\n",
    "            if data_shape[2] > sequence_length:\n",
    "                print(i)\n",
    "                npad = ((0, abs(data_shape[0] - 512)), (0, abs(data_shape[1] - 1024)), (0, 0))\n",
    "                raw_data = np.pad(raw_data, pad_width=npad, mode='edge')\n",
    "                raw_data = np.expand_dims(raw_data, axis=0)\n",
    "                raw_data = block_reduce(raw_data[:,:,:,:], (1,4,4,1),np.max)\n",
    "\n",
    "                data_shape = raw_data.shape\n",
    "                num_time_steps = data_shape[-1] // (n_state_var + 2)\n",
    "                if purpose == \"diff_training\":\n",
    "                    j_range = num_time_steps - sequence_length\n",
    "                else:\n",
    "                    j_range = 1\n",
    "                state_seq_case = [np.concatenate([raw_data[:, :, :192, (j + k) * (n_state_var + 2):\\\n",
    "                                                        (j + k) * (n_state_var + 2) + n_state_var] \\\n",
    "                                                        for k in range(sequence_length)], axis=-1) \\\n",
    "                                                        for j in range  (j_range)] \n",
    "\n",
    "                vel_seq_case = [np.concatenate([raw_data[:, :, :192, (j + k) * (n_state_var + 2) +  n_state_var :\\\n",
    "                                                        (j + k) * (n_state_var + 2) + n_state_var + 2] \\\n",
    "                                                        for k in range(sequence_length)], axis=-1) \\\n",
    "                                                        for j in range (j_range)] \n",
    "\n",
    "                state_seq_whole.extend(state_seq_case)\n",
    "                vel_seq_whole.extend(vel_seq_case)\n",
    "\n",
    "    state_seq_whole = np.concatenate(state_seq_whole, axis=0)\n",
    "    vel_seq_whole = np.concatenate(vel_seq_whole, axis=0)\n",
    "\n",
    "    return state_seq_whole, vel_seq_whole\n",
    "\n",
    "# Normalization\n",
    "def data_normalization(input_data,no_of_channel):\n",
    "    norm_data = np.zeros(input_data.shape)\n",
    "    min_val = []\n",
    "    max_val = []\n",
    "    for i in range(no_of_channel):\n",
    "        norm_data[:,:,:,i::no_of_channel] = ((input_data[:,:,:,i::no_of_channel] - np.amin(input_data[:,:,:,i::no_of_channel])) / (np.amax(input_data[:,:,:,i::no_of_channel]) - np.amin(input_data[:,:,:,i::no_of_channel])) + 1E-9)\n",
    "        min_val.append(np.amin(input_data[:,:,:,i::no_of_channel]))\n",
    "        max_val.append(np.amax(input_data[:,:,:,i::no_of_channel]))\n",
    "    return norm_data, min_val, max_val\n",
    "\n",
    "def data_normalization_test(input_data, min_val, max_val, no_of_channel):\n",
    "    norm_data = np.zeros(input_data.shape)\n",
    "    for i in range(no_of_channel):\n",
    "        norm_data[:,:,:,i::no_of_channel] = ((input_data[:,:,:,i::no_of_channel] - min_val[i]) / (max_val[i] - min_val[i] + 1E-9))\n",
    "    return norm_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_seq_whole, vel_seq_whole = clip_raw_data(idx_range = (150,200), sequence_length = 15, n_state_var = 3, purpose = \"test\")\n",
    "state_seq_norm_test = data_normalization_test(state_seq_whole, state_seq_norm[1], state_seq_norm[2],3)\n",
    "vel_seq_norm_test = data_normalization_test(vel_seq_whole, vel_seq_norm[1], vel_seq_norm[2],2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "parc_rk = model.PARCv2(n_state_var = 3, n_time_step = 14, step_size= 1/15, solver = \"rk4\", mode= \"integrator_training\")\n",
    "parc_rk.compile()\n",
    "parc_rk.differentiator.load_weights('parc2_diff_rk4.h5')\n",
    "parc_rk.integrator.load_weights('parc2_int_rk4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parc_rk.save('parcv2_em.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def data_denormalization(input_data, min_val, max_val, no_of_channel):\n",
    "    norm_data = np.zeros(input_data.shape)\n",
    "    for i in range(no_of_channel):\n",
    "        norm_data[:,:,:,i::no_of_channel] = (input_data[:,:,:,i::no_of_channel] * (max_val[i] - min_val[i] + 1E-9)) + min_val[i]\n",
    "    return norm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_out = data_denormalization(pred,min_val_state,max_val_state, no_of_channel = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.save('./plotting/em/parcv2_em.npy',pred_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantitative analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_rk = []\n",
    "outlier = [0, 3, 30, 31, 32, 33, 18, 19, 11]\n",
    "# outlier = []\n",
    "for idx in range(34):\n",
    "    if idx not in outlier:\n",
    "        print(idx)\n",
    "        state_var_current = state_seq_norm_test[idx:idx+1,:,:,0:3]\n",
    "        velocity_current = vel_seq_norm_test[idx:idx+1,:,:,0:2]\n",
    "        pred_state = parc_rk.predict([state_var_current,velocity_current])\n",
    "        pred_state_case = np.concatenate(pred_state, axis = -1)\n",
    "        print(pred_state_case.shape)\n",
    "        state_rk.append(pred_state_case)\n",
    "state_rk = np.concatenate(state_rk, axis = 0)\n",
    "print(state_rk.shape)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sensitivity(y_true, y_pred_euler, y_pred_rk, metric, ts):\n",
    "    \"\"\"sensitivity plot comparing true and prediction\n",
    "    :param y_true:  (tuple)\n",
    "    :param y_pred:  (tuple)\n",
    "    :param metric:  (str)   metric for plotting. {hs_temp, hs_area, rate_hs_temp, rate_hs_area}\n",
    "    \"\"\"\n",
    "\n",
    "    if metric == \"hs_temp\" or metric == \"hs_area\":\n",
    "        ts = np.linspace(3.16, 15.01, ts)\n",
    "    elif metric == \"rate_hs_temp\" or metric == \"rate_hs_area\":\n",
    "        ts = np.linspace(3.16, 14.22, ts - 1)\n",
    "    else:\n",
    "        print(\n",
    "            \"Wrong metric selection. Possible metrics are: 'hs_temp', 'hs_area', 'rate_hs_temp', 'rate_hs_area\"\n",
    "        )\n",
    "\n",
    "    col_true, col_pred, col_yel = \"#277DA1\", \"#F94144\", \"#F9C74F\"\n",
    "    plt.figure(figsize=(13, 10))\n",
    "\n",
    "    # mean values\n",
    "    plt.plot(ts, y_true[0], color=col_true, lw=2.5, label=\"Ground truth\")\n",
    "    plt.plot(ts, y_pred_rk[0], color=col_pred, lw=2.5, label=\"PARC-RK4\")\n",
    "#     plt.plot(ts, y_pred_euler[0], color=col_yel, lw=2.5, label=\"PARC-Euler\")\n",
    "\n",
    "    # plot intervals\n",
    "    plt.fill_between(ts, y_true[1], y_true[2], color=col_true, alpha=0.2)\n",
    "    plt.fill_between(ts, y_pred_rk[1], y_pred_rk[2], color=col_pred, alpha=0.2)\n",
    "#     plt.fill_between(ts, y_pred_euler[1], y_pred_euler[2], color=col_yel, alpha=0.2)\n",
    "\n",
    "    # corresponding titles and wordings based on the metric\n",
    "    if metric == \"hs_temp\":\n",
    "        plt.title(r\"Ave. Hotspot Temperature ($T_{hs}$)\", fontsize=32, pad=15)\n",
    "        plt.xlabel(r\"t ($ns$)\", fontsize=28)\n",
    "        plt.ylabel(r\" $T_{hs}$ ($K$)\", fontsize=28)\n",
    "        plt.axis([3.16, 15.01, 0, 5000])\n",
    "    elif metric == \"hs_area\":\n",
    "        plt.title(r\"Hotspot Area ($A_{hs}$)\", fontsize=32, pad=15)\n",
    "        plt.xlabel(r\"t ($ns$)\", fontsize=28)\n",
    "        plt.ylabel(r\"$A_{hs}$ \", fontsize=28)\n",
    "        plt.axis([3.16, 15.01, 0, 25])\n",
    "    elif metric == \"rate_hs_temp\":\n",
    "        plt.title(\n",
    "            r\"Ave. Hotspot Temperature Rate of Change ($\\dot{T_{hs}}$)\",\n",
    "            fontsize=32,\n",
    "            pad=15,\n",
    "        )\n",
    "        plt.xlabel(r\"t ($ns$)\", fontsize=28)\n",
    "        plt.ylabel(r\"$\\dot{T_{hs}}$ ($K$/$ns$)\", fontsize=28)\n",
    "        plt.axis([3.16, 15.01, -30, 1200])\n",
    "    else:\n",
    "        plt.title(r\"Hotspot Area Rate of Change ($\\dot{A_{hs}}$)\", fontsize=32, pad=15)\n",
    "        plt.xlabel(r\"t ($ns$)\", fontsize=28)\n",
    "        plt.ylabel(r\"$\\dot{A_{hs}}$\", fontsize=28)\n",
    "        plt.axis([3.16, 15.01, 0, 10])\n",
    "\n",
    "    plt.xticks(fontsize=28)\n",
    "    plt.yticks(fontsize=28)\n",
    "    plt.legend(loc=2, fontsize=28)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocess temperature data\n",
    "# Get temperature field evolution\n",
    "# Temp_gt = np.delete(state_seq_norm_test, outlier, 0)\n",
    "# Temp_gt = Temp_gt[:,:,:,3::3]    # Ground truth temperature field evolution\n",
    "Temp_gt = state_seq_norm_test[:,:,:,3::3]    # Ground truth temperature field evolution\n",
    "# Temp_gt = np.clip(Temp_gt,0,1)\n",
    "Temp_pred_parc_rk = state_rk[:,:,:,0::5]   # PARC prediction temperature field evolution\n",
    "Temp_pred_parc_euler = state_euler[:,:,:,0::5]   # PARC prediction temperature field evolution\n",
    "\n",
    "# # Denormalize temperature field evolution\n",
    "Temp_gt = misc.scale_temperature(temperatures = Temp_gt, start_ts = 0, max_temp = 5000, min_temp = 300)\n",
    "Temp_pred_parc_rk = misc.scale_temperature(temperatures = Temp_pred_parc_rk, start_ts = 0, max_temp = 5000, min_temp = 300)\n",
    "Temp_pred_parc_euler = misc.scale_temperature(temperatures = Temp_pred_parc_euler, start_ts = 0, max_temp = 5000, min_temp = 300)\n",
    "\n",
    "# Compute sensitivity of ground truth data\n",
    "gt_hs_temp, gt_hs_area = metrics.calculate_hotspot_metric(Temp_gt[:,:,:,0:], cases_range = (0,25), n_timesteps = 15)\n",
    "gt_rate_hs_temp,gt_rate_hs_area = metrics.calculate_hotspot_metric_rate_of_change(Temp_gt[:,:,:,0:], cases_range = (0,25), n_timesteps = 15)\n",
    "\n",
    "# Compute sensitivity of PARC prediction\n",
    "parc_hs_temp_rk, parc_hs_area_rk = metrics.calculate_hotspot_metric(Temp_pred_parc_rk[:,:,:,0:], cases_range = (0,25), n_timesteps =15)\n",
    "parc_rate_hs_temp_rk, parc_rate_hs_area_rk = metrics.calculate_hotspot_metric_rate_of_change(Temp_pred_parc_rk[:,:,:,0:], cases_range = (0,25), n_timesteps = 15)\n",
    "\n",
    "# Compute sensitivity of PARC prediction\n",
    "parc_hs_temp_euler, parc_hs_area_euler = metrics.calculate_hotspot_metric(Temp_pred_parc_euler[:,:,:,0:], cases_range = (0,25), n_timesteps =15)\n",
    "parc_rate_hs_temp_euler, parc_rate_hs_area_euler = metrics.calculate_hotspot_metric_rate_of_change(Temp_pred_parc_euler[:,:,:,0:], cases_range = (0,25), n_timesteps = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sensitivity(y_true=gt_hs_temp, y_pred_rk=parc_hs_temp_rk, y_pred_euler = parc_hs_temp_euler,  metric='hs_temp',ts=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sensitivity(y_true=gt_hs_area, y_pred_rk=parc_hs_area_rk, y_pred_euler = parc_hs_area_euler, metric='hs_area',ts=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sensitivity(y_true=gt_rate_hs_temp, y_pred_rk=parc_rate_hs_temp_rk, y_pred_euler = parc_rate_hs_temp_euler, metric='rate_hs_temp',ts=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sensitivity(y_true=gt_rate_hs_area, y_pred_rk=parc_rate_hs_area_rk, y_pred_euler = parc_rate_hs_area_euler, metric='rate_hs_area',ts=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocess temperature data\n",
    "# Get temperature field evolution\n",
    "# Temp_gt = np.delete(state_seq_norm_test, outlier, 0)\n",
    "# Temp_gt = Temp_gt[:,:,:,3::3]    # Ground truth temperature field evolution\n",
    "P_gt = vel_seq_norm_test[:,:,:,4::3]    # Ground truth temperature field evolution\n",
    "# Temp_gt = np.clip(Temp_gt,0,1)\n",
    "P_pred_parc_rk = state_rk[:,:,:,1::5]   # PARC prediction temperature field evolution\n",
    "\n",
    "# # Denormalize temperature field evolution\n",
    "Temp_gt = misc.scale_temperature(temperatures = Temp_gt, start_ts = 0, max_temp = 50e9, min_temp = -2e9)\n",
    "Temp_pred_parc_rk = misc.scale_temperature(temperatures = Temp_pred_parc_rk, start_ts = 0, max_temp = 50e9, min_temp = -2e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sensitivity(y_true=gt_rate_hs_area, y_pred_rk=parc_rate_hs_area_rk, y_pred_euler = parc_rate_hs_area_euler, metric='rate_hs_area',ts=15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow 2.13.0",
   "language": "python",
   "name": "tensorflow-2.13.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
