{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\phong\\.conda\\envs\\tf\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.chdir(\".\")\n",
    "from parc.data.data import EnergeticMatDataPipeLine as EmData\n",
    "from parc import misc, metrics, visualization\n",
    "from parc.model import model_em\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get data and normalization\n",
    "state_seq_whole, vel_seq_whole = EmData.clip_raw_data(idx_range = (0,150))\n",
    "state_seq_norm = EmData.data_normalization(state_seq_whole,3)\n",
    "vel_seq_norm = EmData.data_normalization(vel_seq_whole,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 1: Differentiator training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 10:20:02.287387: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1636] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 78651 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:07:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "# Create tf.dataset\n",
    "dataset_input = tf.data.Dataset.from_tensor_slices((state_seq_norm[0][:,:,:,:3],vel_seq_norm[0][:,:,:,:2]))\n",
    "dataset_label = tf.data.Dataset.from_tensor_slices((state_seq_norm[0][:,:,:,3:],vel_seq_norm[0][:,:,:,2:]))\n",
    "dataset = tf.data.Dataset.zip((dataset_input, dataset_label))\n",
    "dataset = dataset.shuffle(buffer_size = 2192) \n",
    "dataset = dataset.batch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "parc = model_em.PARCv2(n_state_var = 3, n_time_step = 1, step_size= 1/15, solver = \"rk4\", mode = \"differentiator_training\")\n",
    "parc.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.00001, beta_1 = 0.9, beta_2 = 0.999))\n",
    "parc.fit(dataset, epochs = 200, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parc.differentiator.save_weights('parc2_diff_rk4.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2: Data-driven integration training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pretrain integrator\n",
    "tf.keras.backend.clear_session()\n",
    "parc = model_em.PARCv2(n_state_var = 3, n_time_step = 1, step_size= 1/15, solver = \"euler\", mode = \"integrator_training\")\n",
    "parc.differentiator.load_weights('parc2_diff_rk4.h5')\n",
    "parc.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.00001, beta_1 = 0.9, beta_2 = 0.999))\n",
    "parc.fit(dataset, epochs = 100, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parc.integrator.save_weights('parc2_int_rk4.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import skimage\n",
    "from skimage.measure import block_reduce\n",
    "\n",
    "\n",
    "def clip_raw_data(idx_range, sequence_length=2, n_state_var=3, purpose = \"diff_training\"):\n",
    "    state_seq_whole = []\n",
    "    vel_seq_whole = []\n",
    "\n",
    "    for i in range(idx_range[0],idx_range[1]):\n",
    "        file_path = os.path.join(os.sep,'project','SDS','research', 'Nguyen_storage', 'data', 'single_void_data', f'void_{i}.npy')\n",
    "        if os.path.exists(file_path):\n",
    "            raw_data = np.float32(np.load(file_path))\n",
    "            data_shape = raw_data.shape\n",
    "            if data_shape[2] > sequence_length:\n",
    "                print(i)\n",
    "                npad = ((0, abs(data_shape[0] - 512)), (0, abs(data_shape[1] - 1024)), (0, 0))\n",
    "                raw_data = np.pad(raw_data, pad_width=npad, mode='edge')\n",
    "                raw_data = np.expand_dims(raw_data, axis=0)\n",
    "                raw_data = block_reduce(raw_data[:,:,:,:], (1,4,4,1),np.max)\n",
    "\n",
    "                data_shape = raw_data.shape\n",
    "                num_time_steps = data_shape[-1] // (n_state_var + 2)\n",
    "                if purpose == \"diff_training\":\n",
    "                    j_range = num_time_steps - sequence_length\n",
    "                else:\n",
    "                    j_range = 1\n",
    "                state_seq_case = [np.concatenate([raw_data[:, :, :192, (j + k) * (n_state_var + 2):\\\n",
    "                                                        (j + k) * (n_state_var + 2) + n_state_var] \\\n",
    "                                                        for k in range(sequence_length)], axis=-1) \\\n",
    "                                                        for j in range  (j_range)] \n",
    "\n",
    "                vel_seq_case = [np.concatenate([raw_data[:, :, :192, (j + k) * (n_state_var + 2) +  n_state_var :\\\n",
    "                                                        (j + k) * (n_state_var + 2) + n_state_var + 2] \\\n",
    "                                                        for k in range(sequence_length)], axis=-1) \\\n",
    "                                                        for j in range (j_range)] \n",
    "\n",
    "                state_seq_whole.extend(state_seq_case)\n",
    "                vel_seq_whole.extend(vel_seq_case)\n",
    "\n",
    "    state_seq_whole = np.concatenate(state_seq_whole, axis=0)\n",
    "    vel_seq_whole = np.concatenate(vel_seq_whole, axis=0)\n",
    "\n",
    "    return state_seq_whole, vel_seq_whole\n",
    "\n",
    "# Normalization\n",
    "def data_normalization(input_data,no_of_channel):\n",
    "    norm_data = np.zeros(input_data.shape)\n",
    "    min_val = []\n",
    "    max_val = []\n",
    "    for i in range(no_of_channel):\n",
    "        norm_data[:,:,:,i::no_of_channel] = ((input_data[:,:,:,i::no_of_channel] - np.amin(input_data[:,:,:,i::no_of_channel])) / (np.amax(input_data[:,:,:,i::no_of_channel]) - np.amin(input_data[:,:,:,i::no_of_channel])) + 1E-9)\n",
    "        min_val.append(np.amin(input_data[:,:,:,i::no_of_channel]))\n",
    "        max_val.append(np.amax(input_data[:,:,:,i::no_of_channel]))\n",
    "    return norm_data, min_val, max_val\n",
    "\n",
    "def data_normalization_test(input_data, min_val, max_val, no_of_channel):\n",
    "    norm_data = np.zeros(input_data.shape)\n",
    "    for i in range(no_of_channel):\n",
    "        norm_data[:,:,:,i::no_of_channel] = ((input_data[:,:,:,i::no_of_channel] - min_val[i]) / (max_val[i] - min_val[i] + 1E-9))\n",
    "    return norm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_seq_whole, vel_seq_whole = clip_raw_data(idx_range = (150,200), sequence_length = 15, n_state_var = 3, purpose = \"test\")\n",
    "state_seq_norm_test = data_normalization_test(state_seq_whole, state_seq_norm[1], state_seq_norm[2],3)\n",
    "vel_seq_norm_test = data_normalization_test(vel_seq_whole, vel_seq_norm[1], vel_seq_norm[2],2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "parc_rk = model_em.PARCv2(n_state_var = 3, n_time_step = 14, step_size= 1/15, solver = \"rk4\", mode= \"integrator_training\")\n",
    "parc_rk.compile()\n",
    "parc_rk.differentiator.load_weights('parc2_diff_rk4.h5')\n",
    "parc_rk.integrator.load_weights('parc2_int_rk4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parc_rk.save('parcv2_em.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def data_denormalization(input_data, min_val, max_val, no_of_channel):\n",
    "    norm_data = np.zeros(input_data.shape)\n",
    "    for i in range(no_of_channel):\n",
    "        norm_data[:,:,:,i::no_of_channel] = (input_data[:,:,:,i::no_of_channel] * (max_val[i] - min_val[i] + 1E-9)) + min_val[i]\n",
    "    return norm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_out = data_denormalization(pred,min_val_state,max_val_state, no_of_channel = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.save('./plotting/em/parcv2_em.npy',pred_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantitative analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_rk = []\n",
    "outlier = [0, 3, 30, 31, 32, 33, 18, 19, 11]\n",
    "# outlier = []\n",
    "for idx in range(34):\n",
    "    if idx not in outlier:\n",
    "        print(idx)\n",
    "        state_var_current = state_seq_norm_test[idx:idx+1,:,:,0:3]\n",
    "        velocity_current = vel_seq_norm_test[idx:idx+1,:,:,0:2]\n",
    "        pred_state = parc_rk.predict([state_var_current,velocity_current])\n",
    "        pred_state_case = np.concatenate(pred_state, axis = -1)\n",
    "        print(pred_state_case.shape)\n",
    "        state_rk.append(pred_state_case)\n",
    "state_rk = np.concatenate(state_rk, axis = 0)\n",
    "print(state_rk.shape)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sensitivity(y_true, y_pred_euler, y_pred_rk, metric, ts):\n",
    "    \"\"\"sensitivity plot comparing true and prediction\n",
    "    :param y_true:  (tuple)\n",
    "    :param y_pred:  (tuple)\n",
    "    :param metric:  (str)   metric for plotting. {hs_temp, hs_area, rate_hs_temp, rate_hs_area}\n",
    "    \"\"\"\n",
    "\n",
    "    if metric == \"hs_temp\" or metric == \"hs_area\":\n",
    "        ts = np.linspace(3.16, 15.01, ts)\n",
    "    elif metric == \"rate_hs_temp\" or metric == \"rate_hs_area\":\n",
    "        ts = np.linspace(3.16, 14.22, ts - 1)\n",
    "    else:\n",
    "        print(\n",
    "            \"Wrong metric selection. Possible metrics are: 'hs_temp', 'hs_area', 'rate_hs_temp', 'rate_hs_area\"\n",
    "        )\n",
    "\n",
    "    col_true, col_pred, col_yel = \"#277DA1\", \"#F94144\", \"#F9C74F\"\n",
    "    plt.figure(figsize=(13, 10))\n",
    "\n",
    "    # mean values\n",
    "    plt.plot(ts, y_true[0], color=col_true, lw=2.5, label=\"Ground truth\")\n",
    "    plt.plot(ts, y_pred_rk[0], color=col_pred, lw=2.5, label=\"PARC-RK4\")\n",
    "#     plt.plot(ts, y_pred_euler[0], color=col_yel, lw=2.5, label=\"PARC-Euler\")\n",
    "\n",
    "    # plot intervals\n",
    "    plt.fill_between(ts, y_true[1], y_true[2], color=col_true, alpha=0.2)\n",
    "    plt.fill_between(ts, y_pred_rk[1], y_pred_rk[2], color=col_pred, alpha=0.2)\n",
    "#     plt.fill_between(ts, y_pred_euler[1], y_pred_euler[2], color=col_yel, alpha=0.2)\n",
    "\n",
    "    # corresponding titles and wordings based on the metric\n",
    "    if metric == \"hs_temp\":\n",
    "        plt.title(r\"Ave. Hotspot Temperature ($T_{hs}$)\", fontsize=32, pad=15)\n",
    "        plt.xlabel(r\"t ($ns$)\", fontsize=28)\n",
    "        plt.ylabel(r\" $T_{hs}$ ($K$)\", fontsize=28)\n",
    "        plt.axis([3.16, 15.01, 0, 5000])\n",
    "    elif metric == \"hs_area\":\n",
    "        plt.title(r\"Hotspot Area ($A_{hs}$)\", fontsize=32, pad=15)\n",
    "        plt.xlabel(r\"t ($ns$)\", fontsize=28)\n",
    "        plt.ylabel(r\"$A_{hs}$ \", fontsize=28)\n",
    "        plt.axis([3.16, 15.01, 0, 25])\n",
    "    elif metric == \"rate_hs_temp\":\n",
    "        plt.title(\n",
    "            r\"Ave. Hotspot Temperature Rate of Change ($\\dot{T_{hs}}$)\",\n",
    "            fontsize=32,\n",
    "            pad=15,\n",
    "        )\n",
    "        plt.xlabel(r\"t ($ns$)\", fontsize=28)\n",
    "        plt.ylabel(r\"$\\dot{T_{hs}}$ ($K$/$ns$)\", fontsize=28)\n",
    "        plt.axis([3.16, 15.01, -30, 1200])\n",
    "    else:\n",
    "        plt.title(r\"Hotspot Area Rate of Change ($\\dot{A_{hs}}$)\", fontsize=32, pad=15)\n",
    "        plt.xlabel(r\"t ($ns$)\", fontsize=28)\n",
    "        plt.ylabel(r\"$\\dot{A_{hs}}$\", fontsize=28)\n",
    "        plt.axis([3.16, 15.01, 0, 10])\n",
    "\n",
    "    plt.xticks(fontsize=28)\n",
    "    plt.yticks(fontsize=28)\n",
    "    plt.legend(loc=2, fontsize=28)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocess temperature data\n",
    "# Get temperature field evolution\n",
    "# Temp_gt = Temp_gt[:,:,:,3::3]    # Ground truth temperature field evolution\n",
    "Temp_gt = state_seq_norm_test[:,:,:,3::3]    # Ground truth temperature field evolution\n",
    "# Temp_gt = np.clip(Temp_gt,0,1)\n",
    "Temp_pred_parc_rk = state_rk[:,:,:,0::5]   # PARC prediction temperature field evolution\n",
    "\n",
    "# # Denormalize temperature field evolution\n",
    "Temp_gt = misc.scale_temperature(temperatures = Temp_gt, start_ts = 0, max_temp = 5000, min_temp = 300)\n",
    "Temp_pred_parc_rk = misc.scale_temperature(temperatures = Temp_pred_parc_rk, start_ts = 0, max_temp = 5000, min_temp = 300)\n",
    "Temp_pred_parc_euler = misc.scale_temperature(temperatures = Temp_pred_parc_euler, start_ts = 0, max_temp = 5000, min_temp = 300)\n",
    "\n",
    "# Compute sensitivity of ground truth data\n",
    "gt_hs_temp, gt_hs_area = metrics.calculate_hotspot_metric(Temp_gt[:,:,:,0:], cases_range = (0,25), n_timesteps = 15)\n",
    "gt_rate_hs_temp,gt_rate_hs_area = metrics.calculate_hotspot_metric_rate_of_change(Temp_gt[:,:,:,0:], cases_range = (0,25), n_timesteps = 15)\n",
    "\n",
    "# Compute sensitivity of PARC prediction\n",
    "parc_hs_temp_rk, parc_hs_area_rk = metrics.calculate_hotspot_metric(Temp_pred_parc_rk[:,:,:,0:], cases_range = (0,25), n_timesteps =15)\n",
    "parc_rate_hs_temp_rk, parc_rate_hs_area_rk = metrics.calculate_hotspot_metric_rate_of_change(Temp_pred_parc_rk[:,:,:,0:], cases_range = (0,25), n_timesteps = 15)\n",
    "\n",
    "# Compute sensitivity of PARC prediction\n",
    "parc_hs_temp_euler, parc_hs_area_euler = metrics.calculate_hotspot_metric(Temp_pred_parc_euler[:,:,:,0:], cases_range = (0,25), n_timesteps =15)\n",
    "parc_rate_hs_temp_euler, parc_rate_hs_area_euler = metrics.calculate_hotspot_metric_rate_of_change(Temp_pred_parc_euler[:,:,:,0:], cases_range = (0,25), n_timesteps = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sensitivity(y_true=gt_hs_temp, y_pred_rk=parc_hs_temp_rk, y_pred_euler = parc_hs_temp_euler,  metric='hs_temp',ts=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sensitivity(y_true=gt_hs_area, y_pred_rk=parc_hs_area_rk, y_pred_euler = parc_hs_area_euler, metric='hs_area',ts=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sensitivity(y_true=gt_rate_hs_temp, y_pred_rk=parc_rate_hs_temp_rk, y_pred_euler = parc_rate_hs_temp_euler, metric='rate_hs_temp',ts=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sensitivity(y_true=gt_rate_hs_area, y_pred_rk=parc_rate_hs_area_rk, y_pred_euler = parc_rate_hs_area_euler, metric='rate_hs_area',ts=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocess temperature data\n",
    "# Get temperature field evolution\n",
    "# Temp_gt = np.delete(state_seq_norm_test, outlier, 0)\n",
    "# Temp_gt = Temp_gt[:,:,:,3::3]    # Ground truth temperature field evolution\n",
    "P_gt = vel_seq_norm_test[:,:,:,4::3]    # Ground truth temperature field evolution\n",
    "# Temp_gt = np.clip(Temp_gt,0,1)\n",
    "P_pred_parc_rk = state_rk[:,:,:,1::5]   # PARC prediction temperature field evolution\n",
    "\n",
    "# # Denormalize temperature field evolution\n",
    "Temp_gt = misc.scale_temperature(temperatures = Temp_gt, start_ts = 0, max_temp = 50e9, min_temp = -2e9)\n",
    "Temp_pred_parc_rk = misc.scale_temperature(temperatures = Temp_pred_parc_rk, start_ts = 0, max_temp = 50e9, min_temp = -2e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sensitivity(y_true=gt_rate_hs_area, y_pred_rk=parc_rate_hs_area_rk, y_pred_euler = parc_rate_hs_area_euler, metric='rate_hs_area',ts=15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
