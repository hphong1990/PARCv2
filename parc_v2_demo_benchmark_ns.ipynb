{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-18 11:09:56.333802: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.chdir(\".\")\n",
    "# from parc.data import EnergeticMatDataPipeLine as EmData\n",
    "from parc import misc, metrics, model,visualization\n",
    "from parc.model import model_burgers\n",
    "from skimage.measure import block_reduce\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import skimage\n",
    "from skimage.measure import block_reduce\n",
    "\n",
    "\n",
    "Re_list = [15,20,30,40,60,80,100,120,140,150,200,250,300,350,400,450,500,550,600,650,700,750,800,850,900,950,1000,2000,3000,4000,5000,6000,7000,8000,9000,10000]\n",
    "\n",
    "def clip_raw_data():\n",
    "    data_whole = []\n",
    "    # r_whole = []\n",
    "    for Re in Re_list:\n",
    "        data_file_name = 'Re_' + str(int(Re)) + '.npy'\n",
    "#         print(data_file_name)\n",
    "        file_path = './ns_data/' + data_file_name                \n",
    "        if os.path.exists(file_path):\n",
    "            raw_data = np.float32(np.load(file_path))\n",
    "            raw_data = np.expand_dims(raw_data, axis = 0)\n",
    "#             raw_data = skimage.measure.block_reduce(raw_data[:,:,:,:], (1,2,2,1),np.max)\n",
    "            data_shape = raw_data.shape\n",
    "            # print(data_shape)\n",
    "            # norm_r = Re/1000\n",
    "            # r_img = norm_r*np.ones(shape = (data_shape[0],data_shape[1],data_shape[2],1))\n",
    "            # r_whole.extend(r_img)\n",
    "\n",
    "            data_whole.extend(raw_data)\n",
    "\n",
    "    data_whole = np.concatenate([data_whole], axis=0)\n",
    "    # r_whole = np.concatenate([r_whole], axis=0)\n",
    "    return data_whole\n",
    "\n",
    "seq_clipped = clip_raw_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "def data_normalization(input_data,no_of_channel):\n",
    "    norm_data = np.zeros(input_data.shape)\n",
    "    min_val = []\n",
    "    max_val = []\n",
    "    for i in range(no_of_channel):\n",
    "        iter_max_val = np.amax(input_data[:,:,:,i::3])\n",
    "        iter_min_val = np.amin(input_data[:,:,:,i::3])\n",
    "        norm_data[:,:,:,(i)::no_of_channel] = ((input_data[:,:,:,(i)::no_of_channel] - iter_min_val)) / (iter_max_val - iter_min_val + 1E-9)\n",
    "        min_val.append(iter_min_val)\n",
    "        max_val.append(iter_max_val)\n",
    "    return norm_data, min_val, max_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_clipped.shape\n",
    "seq_norm = data_normalization(seq_clipped, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.757973, -2.6737006, -255955.38] [4.0362315, 2.584277, 109451.57]\n"
     ]
    }
   ],
   "source": [
    "print(seq_norm[1], seq_norm[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Re_list = [15,20,30,40,60,80,100,120,140,150,200,250,300,350,400,450,500,550,600,650,700,750,800,850,900,950,1000,2000,3000,4000,5000,6000,7000,8000,9000,10000]\n",
    "train_list = [30,40,80,100,150,200,250,300,400,450,500,600,650,700,800,850,900,950]\n",
    "test_list = [20,60,140,350,550,750,1000]\n",
    "idx = 0\n",
    "train_idx =[]\n",
    "test_idx =[]\n",
    "for Re in Re_list:\n",
    "    if Re in train_list:\n",
    "        train_idx.append(idx)\n",
    "    elif Re in test_list:\n",
    "        test_idx.append(idx)\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 8, 13, 17, 21, 26]\n"
     ]
    }
   ],
   "source": [
    "print(test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = [seq_norm[0][idx:idx+1,:,:,:] for idx in train_idx]\n",
    "# train_re =[seq_clipped[1][idx:idx+1,:,:,:] for idx in train_idx]\n",
    "\n",
    "test_seq = [seq_norm[0][idx:idx+1,:,:,:] for idx in test_idx]\n",
    "# test_re =[seq_clipped[1][idx:idx+1,:,:,:] for idx in test_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 128, 256, 117)\n"
     ]
    }
   ],
   "source": [
    "train_seq = np.concatenate(train_seq, axis = 0)\n",
    "# train_re = np.concatenate(train_re, axis = 0)\n",
    "\n",
    "test_seq = np.concatenate(test_seq, axis = 0)\n",
    "# test_re = np.concatenate(test_re, axis = 0)\n",
    "print(test_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18, 128, 256, 117)\n"
     ]
    }
   ],
   "source": [
    "print(train_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_data(seq, no_of_fields, sequence_length = 2):\n",
    "    shape = seq.shape\n",
    "    num_time_steps = np.int32((shape[-1]-1)/3)\n",
    "    vel_seq_whole = []\n",
    "    # re_seq_whole = []\n",
    "#     constant_whole = []\n",
    "    for i in range(shape[0]):\n",
    "#         constant = seq[i:i+1,:,:,0:1]\n",
    "        for j in range(num_time_steps-sequence_length+1):\n",
    "            vel_seq_case = np.expand_dims(seq[i, :, :, (j*no_of_fields):(j*no_of_fields+sequence_length*no_of_fields)],axis = 0)\n",
    "#             vel_seq_case = np.concatenate([constant,vel_seq_case],axis = -1)\n",
    "            vel_seq_whole.extend(vel_seq_case)\n",
    "            # re_seq_whole.extend(re[i:i+1,:,:,:])\n",
    "    vel_seq_whole = np.concatenate([vel_seq_whole], axis=0)\n",
    "    # re_seq_whole = np.concatenate([re_seq_whole], axis=0)\n",
    "\n",
    "    return vel_seq_whole\n",
    "\n",
    "train_data = create_train_data(train_seq, no_of_fields = 3, sequence_length = 13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 1: Differentiator training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-06 14:52:11.572250: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1636] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 78791 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:0f:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "# Create tf.dataset\n",
    "dataset_input = tf.data.Dataset.from_tensor_slices(train_data[:,:,:,:3])\n",
    "dataset_label = tf.data.Dataset.from_tensor_slices(train_data[:,:,:,3:])\n",
    "dataset = tf.data.Dataset.zip((dataset_input, dataset_label))\n",
    "dataset = dataset.shuffle(buffer_size = 798) \n",
    "dataset = dataset.batch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "parc = PARCv2_ns(n_time_step = 12, step_size= 1/38, solver = \"heun\", mode = \"differentiator_training\")\n",
    "parc.differentiator.load_weights('parc2_diff_ns_heun_10_2.h5')\n",
    "parc.poisson.load_weights('parc2_poisson_ns_heun_10_2.h5')\n",
    "parc.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.000001, beta_1 = 0.5, beta_2 = 0.999))\n",
    "parc.fit(dataset, epochs = 50, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "parc.differentiator.save_weights('parc2_diff_ns_heun_12.h5')\n",
    "parc.poisson.save_weights('parc2_poisson_ns_heun_12.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "parc = PARCv2_ns(n_time_step = 12, step_size= 1/38, solver = \"heun\", mode = \"integrator_training\")\n",
    "parc.differentiator.load_weights('parc2_diff_ns_heun_12.h5')\n",
    "parc.poisson.load_weights('parc2_poisson_ns_heun_12.h5')\n",
    "parc.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.000001, beta_1 = 0.5, beta_2 = 0.999))\n",
    "parc.fit(dataset, epochs = 50, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parc.integrator.save_weights('parc2_int_ns_heun_12.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "parc = PARCv2_ns(n_time_step = 37, step_size= 1/38, solver = \"heun\", use_data_driven_int = False, mode = \"differentiator_training\")\n",
    "parc.differentiator.load_weights('parc2_diff_ns_heun_12.h5')\n",
    "parc.poisson.load_weights('parc2_poisson_ns_heun_12.h5')\n",
    "parc.integrator.load_weights('parc2_int_ns_heun_10.h5')\n",
    "parc.compile()\n",
    "parc.build(input_shape = (None,128,256,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"par_cv2_ns\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " model_2 (Functional)        (None, 128, 256, 2)       15187522  \n",
      "                                                                 \n",
      " model_3 (Functional)        (None, 128, 256, 1)       229313    \n",
      "                                                                 \n",
      " model_4 (Functional)        (None, 128, 256, 2)       439042    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15855879 (60.49 MB)\n",
      "Trainable params: 15416835 (58.81 MB)\n",
      "Non-trainable params: 439044 (1.67 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "parc.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parc.save('parc2_ns_heun.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Skipping variable loading for optimizer 'RMSprop', because it has 97 variables whereas the saved optimizer has 1 variables. \n"
     ]
    }
   ],
   "source": [
    "loaded_parc = tf.keras.models.load_model('parc2_ns_heun.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-07 10:45:56.534022: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8904\n",
      "2024-01-07 10:45:57.343827: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:625] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 38s 38s/step\n",
      "1/1 [==============================] - 0s 250ms/step\n",
      "1/1 [==============================] - 0s 249ms/step\n",
      "1/1 [==============================] - 0s 249ms/step\n",
      "1/1 [==============================] - 0s 249ms/step\n",
      "1/1 [==============================] - 0s 248ms/step\n",
      "1/1 [==============================] - 0s 248ms/step\n"
     ]
    }
   ],
   "source": [
    "pred_whole =[]\n",
    "for idx in range(7):\n",
    "    input_seq_current = tf.cast(test_seq[idx:idx+1,:,:,:3], dtype = tf.float32)\n",
    "    output = loaded_parc.predict(input_seq_current)\n",
    "    pred_whole.append(output)\n",
    "pred = np.concatenate(pred_whole,axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0362315 -2.757973\n",
      "2.584277 -2.6737006\n",
      "109451.57 -255955.38\n"
     ]
    }
   ],
   "source": [
    "def DeNormalization(y_pred, min_val, max_val, no_of_channel):\n",
    "    denorm_data = np.zeros(y_pred.shape)\n",
    "    \n",
    "    for i in range(no_of_channel):\n",
    "        print(max_val[i], min_val[i])\n",
    "        denorm_data[:,:,:,(i)::no_of_channel] = (y_pred[:,:,:,(i)::no_of_channel] * (max_val[i] - min_val[i])) + min_val[i]\n",
    "    return denorm_data\n",
    "\n",
    "y_pred_denorm = DeNormalization(pred,seq_norm[1], seq_norm[2], no_of_channel = 3)\n",
    "# gt_denorm = DeNormalization(test_seq,seq_norm[1], seq_norm[2], no_of_channel = 3)\n",
    "# print(np.amax(np.sqrt(gt_denorm[3,:,:,0::3]**2 + gt_denorm[3,:,:,1::3]**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.save('./plotting/ns/parcv2_ns.npy',y_pred_denorm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow 2.13.0",
   "language": "python",
   "name": "tensorflow-2.13.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
