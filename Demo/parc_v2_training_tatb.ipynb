{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-30 11:50:53.691776: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-30 11:50:58.712401: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1636] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 78791 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:bd:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.chdir(\"/sfs/qumulo/qhome/jtb3sud/PARCv2\")\n",
    "import parc.data.data_em as data\n",
    "#from parc import misc, metrics, visualization\n",
    "from parc.model import model_em as model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 5793831.npy\n",
      "Processing 5733397.npy\n",
      "Processing 5733384.npy\n",
      "Processing 5733386.npy\n",
      "Processing 5793832.npy\n",
      "Processing 5733402.npy\n",
      "Processing 5793835.npy\n",
      "Processing 5793836.npy\n",
      "Processing 5733392.npy\n",
      "Processing 5733388.npy\n",
      "Processing 5733398.npy\n",
      "Processing 5733389.npy\n",
      "Processing 5733401.npy\n",
      "Processing 5793837.npy\n",
      "Processing 5793825.npy\n",
      "Processing 5733385.npy\n",
      "Processing 5793827.npy\n",
      "Processing 5733394.npy\n",
      "Processing 5793834.npy\n",
      "Processing 5793840.npy\n",
      "Processing 5793838.npy\n",
      "Processing 5793826.npy\n",
      "Processing 5733387.npy\n",
      "Processing 5793830.npy\n",
      "Processing 5733390.npy\n",
      "Processing 5733391.npy\n",
      "Processing 5793829.npy\n",
      "Processing 5793839.npy\n",
      "Processing 5733400.npy\n",
      "Processing 5733396.npy\n",
      "Processing 5793833.npy\n"
     ]
    }
   ],
   "source": [
    "# Get data and normalization\n",
    "state_seq_whole, vel_seq_whole = data.clip_raw_data(idx_range = (0,150))\n",
    "state_seq_norm = data.data_normalization(state_seq_whole,3)\n",
    "vel_seq_norm = data.data_normalization(vel_seq_whole,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 1: Differentiator training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tf.dataset\n",
    "dataset_input = tf.data.Dataset.from_tensor_slices((state_seq_norm[0][:,:,:,:3],vel_seq_norm[0][:,:,:,:2]))\n",
    "dataset_label = tf.data.Dataset.from_tensor_slices((state_seq_norm[0][:,:,:,-3:],vel_seq_norm[0][:,:,:,-2:]))\n",
    "dataset = tf.data.Dataset.zip((dataset_input, dataset_label))\n",
    "dataset = dataset.shuffle(buffer_size = 2192) \n",
    "dataset = dataset.batch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_BatchDataset element_spec=((TensorSpec(shape=(None, 128, 208, 3), dtype=tf.float64, name=None), TensorSpec(shape=(None, 128, 208, 2), dtype=tf.float64, name=None)), (TensorSpec(shape=(None, 128, 208, 3), dtype=tf.float64, name=None), TensorSpec(shape=(None, 128, 208, 2), dtype=tf.float64, name=None)))>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 22:19:58.986912: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8904\n",
      "2024-04-29 22:20:07.227805: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f88adf88c30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-04-29 22:20:07.227861: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100-SXM4-80GB, Compute Capability 8.0\n",
      "2024-04-29 22:20:07.325035: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-04-29 22:20:08.102946: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232/232 [==============================] - 273s 958ms/step - total_loss: 185.7712\n",
      "Epoch 2/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 181.2075\n",
      "Epoch 3/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 177.8370\n",
      "Epoch 4/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 176.5101\n",
      "Epoch 5/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 174.7990\n",
      "Epoch 6/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 171.2470\n",
      "Epoch 7/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 169.4326\n",
      "Epoch 8/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 167.6857\n",
      "Epoch 9/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 164.7664\n",
      "Epoch 10/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 163.3846\n",
      "Epoch 11/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 162.3020\n",
      "Epoch 12/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 162.0475\n",
      "Epoch 13/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 160.9416\n",
      "Epoch 14/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 157.8061\n",
      "Epoch 15/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 154.5966\n",
      "Epoch 16/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 155.4049\n",
      "Epoch 17/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 154.7241\n",
      "Epoch 18/100\n",
      "232/232 [==============================] - 222s 956ms/step - total_loss: 153.2914\n",
      "Epoch 19/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 154.0917\n",
      "Epoch 20/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 151.8495\n",
      "Epoch 21/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 151.0116\n",
      "Epoch 22/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 148.2799\n",
      "Epoch 23/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 149.2899\n",
      "Epoch 24/100\n",
      "232/232 [==============================] - 222s 956ms/step - total_loss: 148.2401\n",
      "Epoch 25/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 144.4799\n",
      "Epoch 26/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 146.9205\n",
      "Epoch 27/100\n",
      "232/232 [==============================] - 221s 955ms/step - total_loss: 146.4167\n",
      "Epoch 28/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 141.8543\n",
      "Epoch 29/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 142.5821\n",
      "Epoch 30/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 140.9109\n",
      "Epoch 31/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 140.5969\n",
      "Epoch 32/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 140.0462\n",
      "Epoch 33/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 141.5808\n",
      "Epoch 34/100\n",
      "232/232 [==============================] - 221s 955ms/step - total_loss: 139.4296\n",
      "Epoch 35/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 138.3063\n",
      "Epoch 36/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 139.2652\n",
      "Epoch 37/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 137.2314\n",
      "Epoch 38/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 136.7171\n",
      "Epoch 39/100\n",
      "232/232 [==============================] - 221s 955ms/step - total_loss: 135.5016\n",
      "Epoch 40/100\n",
      "232/232 [==============================] - 221s 955ms/step - total_loss: 135.7393\n",
      "Epoch 41/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 134.6723\n",
      "Epoch 42/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 134.8967\n",
      "Epoch 43/100\n",
      "232/232 [==============================] - 221s 955ms/step - total_loss: 131.6638\n",
      "Epoch 44/100\n",
      "232/232 [==============================] - 221s 955ms/step - total_loss: 134.3115\n",
      "Epoch 45/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 132.8874\n",
      "Epoch 46/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 130.1057\n",
      "Epoch 47/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 133.2096\n",
      "Epoch 48/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 131.6659\n",
      "Epoch 49/100\n",
      "232/232 [==============================] - 221s 955ms/step - total_loss: 131.3380\n",
      "Epoch 50/100\n",
      "232/232 [==============================] - 221s 954ms/step - total_loss: 129.3903\n",
      "Epoch 51/100\n",
      "232/232 [==============================] - 221s 955ms/step - total_loss: 127.9187\n",
      "Epoch 52/100\n",
      "232/232 [==============================] - 221s 955ms/step - total_loss: 127.3668\n",
      "Epoch 53/100\n",
      "232/232 [==============================] - 221s 954ms/step - total_loss: 129.1382\n",
      "Epoch 54/100\n",
      "232/232 [==============================] - 221s 954ms/step - total_loss: 126.7276\n",
      "Epoch 55/100\n",
      "232/232 [==============================] - 221s 954ms/step - total_loss: 127.4745\n",
      "Epoch 56/100\n",
      "232/232 [==============================] - 221s 955ms/step - total_loss: 127.6359\n",
      "Epoch 57/100\n",
      "232/232 [==============================] - 221s 955ms/step - total_loss: 125.1698\n",
      "Epoch 58/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 125.7800\n",
      "Epoch 59/100\n",
      "232/232 [==============================] - 221s 954ms/step - total_loss: 125.6524\n",
      "Epoch 60/100\n",
      "232/232 [==============================] - 221s 955ms/step - total_loss: 122.8787\n",
      "Epoch 61/100\n",
      "232/232 [==============================] - 221s 954ms/step - total_loss: 122.4640\n",
      "Epoch 62/100\n",
      "232/232 [==============================] - 221s 955ms/step - total_loss: 123.3434\n",
      "Epoch 63/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 122.6961\n",
      "Epoch 64/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 124.3551\n",
      "Epoch 65/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 123.2333\n",
      "Epoch 66/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 120.8610\n",
      "Epoch 67/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 121.4893\n",
      "Epoch 68/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 120.8918\n",
      "Epoch 69/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 120.7812\n",
      "Epoch 70/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 120.0460\n",
      "Epoch 71/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 119.6354\n",
      "Epoch 72/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 118.9092\n",
      "Epoch 73/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 119.0244\n",
      "Epoch 74/100\n",
      "232/232 [==============================] - 221s 954ms/step - total_loss: 118.7407\n",
      "Epoch 75/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 119.1743\n",
      "Epoch 76/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 117.8214\n",
      "Epoch 77/100\n",
      "232/232 [==============================] - 221s 955ms/step - total_loss: 115.9532\n",
      "Epoch 78/100\n",
      "232/232 [==============================] - 221s 954ms/step - total_loss: 115.3260\n",
      "Epoch 79/100\n",
      "232/232 [==============================] - 221s 955ms/step - total_loss: 116.3290\n",
      "Epoch 80/100\n",
      "232/232 [==============================] - 221s 955ms/step - total_loss: 117.7606\n",
      "Epoch 81/100\n",
      "232/232 [==============================] - 221s 954ms/step - total_loss: 117.5481\n",
      "Epoch 82/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 115.4378\n",
      "Epoch 83/100\n",
      "232/232 [==============================] - 221s 955ms/step - total_loss: 114.7051\n",
      "Epoch 84/100\n",
      "232/232 [==============================] - 221s 954ms/step - total_loss: 115.8761\n",
      "Epoch 85/100\n",
      "232/232 [==============================] - 221s 955ms/step - total_loss: 115.1228\n",
      "Epoch 86/100\n",
      "232/232 [==============================] - 221s 955ms/step - total_loss: 115.0900\n",
      "Epoch 87/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 112.7493\n",
      "Epoch 88/100\n",
      "232/232 [==============================] - 221s 955ms/step - total_loss: 113.4145\n",
      "Epoch 89/100\n",
      "232/232 [==============================] - 221s 955ms/step - total_loss: 113.6719\n",
      "Epoch 91/100\n",
      "232/232 [==============================] - 221s 954ms/step - total_loss: 112.8195\n",
      "Epoch 92/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 113.8960\n",
      "Epoch 93/100\n",
      "232/232 [==============================] - 221s 955ms/step - total_loss: 112.6448\n",
      "Epoch 94/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 110.0381\n",
      "Epoch 95/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 111.5233\n",
      "Epoch 96/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 111.3972\n",
      "Epoch 97/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 110.8980\n",
      "Epoch 98/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 110.7516\n",
      "Epoch 99/100\n",
      "232/232 [==============================] - 222s 955ms/step - total_loss: 109.9421\n",
      "Epoch 100/100\n",
      "232/232 [==============================] - 221s 955ms/step - total_loss: 109.7483\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f9edc131750>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "# step size needs to be changed to 1/total number of steps\n",
    "parc = model.PARCv2(n_state_var = 3, n_time_step = 1, step_size= 1/60, solver = \"rk4\", mode = \"differentiator_training\")\n",
    "parc.differentiator.load_weights('parc2_diff_rk4_tatb.h5')\n",
    "parc.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.00001, beta_1 = 0.9, beta_2 = 0.999))\n",
    "parc.fit(dataset, epochs = 100, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parc.differentiator.save_weights('parc2_diff_rk4_tatb.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2: Data-driven integration training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-30 11:52:17.496255: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8904\n",
      "2024-04-30 11:52:20.861035: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c617b17e00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-04-30 11:52:20.861077: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100-SXM4-80GB, Compute Capability 8.0\n",
      "2024-04-30 11:52:20.974633: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-04-30 11:52:21.937055: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232/232 [==============================] - 154s 498ms/step - total_loss: 686.6307\n",
      "Epoch 2/125\n",
      "232/232 [==============================] - 115s 494ms/step - total_loss: 567.7000\n",
      "Epoch 3/125\n",
      "232/232 [==============================] - 115s 494ms/step - total_loss: 545.1809\n",
      "Epoch 4/125\n",
      "232/232 [==============================] - 115s 494ms/step - total_loss: 523.2788\n",
      "Epoch 5/125\n",
      "232/232 [==============================] - 115s 494ms/step - total_loss: 514.4716\n",
      "Epoch 6/125\n",
      "232/232 [==============================] - 115s 494ms/step - total_loss: 504.3910\n",
      "Epoch 7/125\n",
      "232/232 [==============================] - 115s 494ms/step - total_loss: 494.5680\n",
      "Epoch 8/125\n",
      "232/232 [==============================] - 115s 494ms/step - total_loss: 488.8584\n",
      "Epoch 9/125\n",
      "232/232 [==============================] - 115s 494ms/step - total_loss: 486.2922\n",
      "Epoch 10/125\n",
      "232/232 [==============================] - 115s 494ms/step - total_loss: 484.7857\n",
      "Epoch 11/125\n",
      "232/232 [==============================] - 115s 494ms/step - total_loss: 482.5334\n",
      "Epoch 12/125\n",
      "232/232 [==============================] - 115s 494ms/step - total_loss: 483.2994\n",
      "Epoch 13/125\n",
      "232/232 [==============================] - 115s 494ms/step - total_loss: 479.1026\n",
      "Epoch 14/125\n",
      "232/232 [==============================] - 115s 494ms/step - total_loss: 470.9211\n",
      "Epoch 15/125\n",
      "232/232 [==============================] - 115s 494ms/step - total_loss: 476.1097\n",
      "Epoch 16/125\n",
      "232/232 [==============================] - 115s 494ms/step - total_loss: 474.2356\n",
      "Epoch 17/125\n",
      "232/232 [==============================] - 115s 494ms/step - total_loss: 474.5525\n",
      "Epoch 18/125\n",
      "232/232 [==============================] - 115s 494ms/step - total_loss: 488.5805\n",
      "Epoch 19/125\n",
      "232/232 [==============================] - 115s 494ms/step - total_loss: 465.3564\n",
      "Epoch 20/125\n",
      "232/232 [==============================] - 115s 494ms/step - total_loss: 478.5482\n",
      "Epoch 21/125\n",
      "232/232 [==============================] - 115s 494ms/step - total_loss: 469.6801\n",
      "Epoch 22/125\n",
      "232/232 [==============================] - 115s 494ms/step - total_loss: 468.0078\n",
      "Epoch 23/125\n",
      "232/232 [==============================] - 115s 494ms/step - total_loss: 464.9856\n",
      "Epoch 24/125\n",
      "232/232 [==============================] - 115s 494ms/step - total_loss: 463.4934\n",
      "Epoch 25/125\n",
      "232/232 [==============================] - 115s 494ms/step - total_loss: 461.5241\n",
      "Epoch 26/125\n",
      "232/232 [==============================] - 115s 494ms/step - total_loss: 460.8741\n",
      "Epoch 27/125\n",
      "232/232 [==============================] - 115s 494ms/step - total_loss: 458.2888\n",
      "Epoch 28/125\n",
      "232/232 [==============================] - 115s 494ms/step - total_loss: 461.5765\n",
      "Epoch 29/125\n",
      "121/232 [==============>...............] - ETA: 54s - total_loss: 448.7494"
     ]
    }
   ],
   "source": [
    "# Pretrain integrator\n",
    "tf.keras.backend.clear_session()\n",
    "parc = model.PARCv2(n_state_var = 3, n_time_step = 1, step_size= 1/60, solver = \"rk4\", mode = \"integrator_training\")\n",
    "parc.integrator.load_weights('parc2_int_rk4_tatb.h5')\n",
    "parc.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.00001, beta_1 = 0.9, beta_2 = 0.999))\n",
    "parc.fit(dataset, epochs = 125, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parc.integrator.save_weights('parc2_int_rk4_tatb.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow 2.13.0",
   "language": "python",
   "name": "tensorflow-2.13.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
