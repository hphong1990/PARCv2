{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-18 11:09:56.333802: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.chdir(\"./..\")\n",
    "from PARC.data import data_navier_stokes as data\n",
    "from PARC.model import model_ns as model\n",
    "from PARC import misc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Re_list = [15,20,30,40,60,80,100,120,140,150,200,250,300,350,400,450,500,550,600,650,700,750,800,850,900,950,1000,2000,3000,4000,5000,6000,7000,8000,9000,10000]\n",
    "train_list = [30,40,80,100,150,200,250,300,400,450,500,600,650,700,800,850,900,950]\n",
    "test_list = [20,60,140,350,550,750,1000]\n",
    "\n",
    "data_ns = data.DataNS(Re_list)\n",
    "seq_input = data_ns.import_data(folder_path='./data/ns/')\n",
    "seq_norm = data_ns.data_normalization(input_data=seq_input, no_of_channel=3)\n",
    "\n",
    "train_seq, test_seq = data_ns.train_test_split(Re_list=Re_list, Re_test=test_list, Re_train=train_list,input_data=seq_norm[0])\n",
    "train_data = data_ns.clip_data(input_seq=train_seq,no_of_fields=3,sequence_length=13)\n",
    "# def clip_raw_data():\n",
    "#     data_whole = []\n",
    "#     # r_whole = []\n",
    "#     for Re in Re_list:\n",
    "#         data_file_name = 'Re_' + str(int(Re)) + '.npy'\n",
    "#         file_path = './data/ns/' + data_file_name                \n",
    "#         if os.path.exists(file_path):\n",
    "#             raw_data = np.float32(np.load(file_path))\n",
    "#             raw_data = np.expand_dims(raw_data, axis = 0)\n",
    "#             data_whole.extend(raw_data)\n",
    "#     data_whole = np.concatenate([data_whole], axis=0)\n",
    "#     return data_whole\n",
    "# seq_clipped = clip_raw_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Normalization\n",
    "# def data_normalization(input_data,no_of_channel):\n",
    "#     norm_data = np.zeros(input_data.shape)\n",
    "#     min_val = []\n",
    "#     max_val = []\n",
    "#     for i in range(no_of_channel):\n",
    "#         iter_max_val = np.amax(input_data[:,:,:,i::3])\n",
    "#         iter_min_val = np.amin(input_data[:,:,:,i::3])\n",
    "#         norm_data[:,:,:,(i)::no_of_channel] = ((input_data[:,:,:,(i)::no_of_channel] - iter_min_val)) / (iter_max_val - iter_min_val + 1E-9)\n",
    "#         min_val.append(iter_min_val)\n",
    "#         max_val.append(iter_max_val)\n",
    "#     return norm_data, min_val, max_val\n",
    "# seq_norm = data_normalization(seq_clipped, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re_list = [15,20,30,40,60,80,100,120,140,150,200,250,300,350,400,450,500,550,600,650,700,750,800,850,900,950,1000,2000,3000,4000,5000,6000,7000,8000,9000,10000]\n",
    "# train_list = [30,40,80,100,150,200,250,300,400,450,500,600,650,700,800,850,900,950]\n",
    "# test_list = [20,60,140,350,550,750,1000]\n",
    "# idx = 0\n",
    "# train_idx =[]\n",
    "# test_idx =[]\n",
    "# for Re in Re_list:\n",
    "#     if Re in train_list:\n",
    "#         train_idx.append(idx)\n",
    "#     elif Re in test_list:\n",
    "#         test_idx.append(idx)\n",
    "#     idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_seq = [seq_norm[0][idx:idx+1,:,:,:] for idx in train_idx]\n",
    "# test_seq = [seq_norm[0][idx:idx+1,:,:,:] for idx in test_idx]\n",
    "# train_seq = np.concatenate(train_seq, axis = 0)\n",
    "# test_seq = np.concatenate(test_seq, axis = 0)\n",
    "# print(test_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_train_data(seq, no_of_fields, sequence_length = 2):\n",
    "#     shape = seq.shape\n",
    "#     num_time_steps = np.int32((shape[-1]-1)/3)\n",
    "#     vel_seq_whole = []\n",
    "#     for i in range(shape[0]):\n",
    "#         for j in range(num_time_steps-sequence_length+1):\n",
    "#             vel_seq_case = np.expand_dims(seq[i, :, :, (j*no_of_fields):(j*no_of_fields+sequence_length*no_of_fields)],axis = 0)\n",
    "#             vel_seq_whole.extend(vel_seq_case)\n",
    "#     vel_seq_whole = np.concatenate([vel_seq_whole], axis=0)\n",
    "#     return vel_seq_whole\n",
    "# train_data = create_train_data(train_seq, no_of_fields = 3, sequence_length = 13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 1: Differentiator training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-06 14:52:11.572250: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1636] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 78791 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:0f:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "# Create tf.dataset\n",
    "dataset_input = tf.data.Dataset.from_tensor_slices(train_data[:,:,:,:3])\n",
    "dataset_label = tf.data.Dataset.from_tensor_slices(train_data[:,:,:,3:])\n",
    "dataset = tf.data.Dataset.zip((dataset_input, dataset_label))\n",
    "dataset = dataset.shuffle(buffer_size = 798) \n",
    "dataset = dataset.batch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "parc = model.PARCv2_ns(n_time_step = 12, step_size= 1/38, solver = \"heun\", mode = \"differentiator_training\")\n",
    "# parc.differentiator.load_weights('parc2_diff_ns_heun_10_2.h5')\n",
    "# parc.poisson.load_weights('parc2_poisson_ns_heun_10_2.h5')\n",
    "parc.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.000001, beta_1 = 0.5, beta_2 = 0.999))\n",
    "parc.fit(dataset, epochs = 500, shuffle = True)\n",
    "\n",
    "parc.differentiator.save_weights('PARC2_NS_diff_heunn12.h5')\n",
    "parc.poisson.save_weights('PARC2_NS_diff_poisson_heunn12.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2: Integrator training (Can skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "parc = model.PARCv2_ns(n_time_step = 12, step_size= 1/38, solver = \"heun\", mode = \"integrator_training\")\n",
    "parc.differentiator.load_weights('parc2_diff_ns_heun_12.h5')\n",
    "parc.poisson.load_weights('parc2_poisson_ns_heun_12.h5')\n",
    "parc.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.000001, beta_1 = 0.5, beta_2 = 0.999))\n",
    "parc.fit(dataset, epochs = 300, shuffle = True)\n",
    "\n",
    "parc.integrator.save_weights('PARC2_NS_int_heunn12.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "parc = model.PARCv2_ns(n_time_step = 37, step_size= 1/38, solver = \"heun\", use_data_driven_int = False, mode = \"differentiator_training\")\n",
    "\n",
    "misc.geturl(url=\"https://virginia.box.com/shared/static/u6msftownrqjzlwhysmm24y4bxc11pdr\", title=\"PARC2_NS_int_heunn12.h5\")\n",
    "misc.geturl(url=\"https://virginia.box.com/shared/static/edusf1diccn8yle0n9k90w5he4zdma54\", title=\"PARC2_NS_diff_heunn12.h5\")\n",
    "misc.geturl(url=\"https://virginia.box.com/shared/static/cyhbtqanwwz0msdod2robstatlvk9faa\", title=\"PARC2_NS_diff_poisson_heunn12.h5\")\n",
    "\n",
    "parc.differentiator.load_weights('parc2_diff_ns_heun_12.h5')\n",
    "parc.poisson.load_weights('parc2_poisson_ns_heun_12.h5')\n",
    "parc.integrator.load_weights('parc2_int_ns_heun_10.h5')\n",
    "parc.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-07 10:45:56.534022: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8904\n",
      "2024-01-07 10:45:57.343827: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:625] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 38s 38s/step\n",
      "1/1 [==============================] - 0s 250ms/step\n",
      "1/1 [==============================] - 0s 249ms/step\n",
      "1/1 [==============================] - 0s 249ms/step\n",
      "1/1 [==============================] - 0s 249ms/step\n",
      "1/1 [==============================] - 0s 248ms/step\n",
      "1/1 [==============================] - 0s 248ms/step\n"
     ]
    }
   ],
   "source": [
    "pred_whole =[]\n",
    "for idx in range(7):\n",
    "    input_seq_current = tf.cast(test_seq[idx:idx+1,:,:,:3], dtype = tf.float32)\n",
    "    output = parc.predict(input_seq_current)\n",
    "    pred_whole.append(output)\n",
    "pred = np.concatenate(pred_whole,axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred_denorm = data_ns.data_denormalization(pred,seq_norm[1], seq_norm[2], no_of_channel = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.save('./plotting/ns/parcv2_ns.npy',y_pred_denorm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load result data\n",
    "gt = np.load('../result/ns/gt.npy')\n",
    "# NeuralODE\n",
    "parc = np.load('../result/ns/parc_ns.npy')\n",
    "# NeuralODE\n",
    "parc_data = np.load('../result/ns/parc_ns_data_int.npy')\n",
    "# FNO\n",
    "fno = np.load('../result/ns/fno_ns.npy')\n",
    "# PhyCRNet\n",
    "phycrnet = np.load('../result/ns/phycrnet_ns.npy')\n",
    "# PI-FNO\n",
    "pifno = np.load('../result/ns/pifno_ns.npy')\n",
    "# PARCv2\n",
    "parcv2 = np.load('../result/ns/parcv2_ns_new.npy')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Re list\n",
    "Re_list = [20, 60, 140, 350, 550, 750, 1000]\n",
    "\n",
    "def turn_off_axis(ax):\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticklabels([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the pressure field\n",
    "for Re in Re_list:\n",
    "    gt[:,:,:,2::3] /= Re*4\n",
    "    parc[:,:,:,2::3] /= Re*4\n",
    "    fno[:,:,:,2::3] /= Re*4\n",
    "    phycrnet[:,:,:,2::3] /= Re*4\n",
    "    pifno[:,:,:,2::3] /= Re*4\n",
    "    parc_data[:,:,:,2::3] /= Re*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [parcv2, parc, parc_data, fno, phycrnet, pifno]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masking the cylinder area\n",
    "rr = 0.125 + 1.0/128\n",
    "#rr = 0.0\n",
    "x, y = np.linspace(0.0, 1.0, 128), np.linspace(0.0, 2.0, 256)\n",
    "xx, yy = np.meshgrid(x, y)\n",
    "mask = (((xx-0.5)**2+(yy-0.5)**2)<=rr*rr)\n",
    "\n",
    "expanded_mask = np.expand_dims(np.expand_dims(mask.transpose(), axis=0), axis=-1)\n",
    "\n",
    "# Set corresponding elements in train_data to 0\n",
    "for i in range(6):\n",
    "    y_pred[i] = np.where(expanded_mask, np.zeros_like(y_pred[i]), y_pred[i])\n",
    "gt = np.where(expanded_mask, np.zeros_like(gt), gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot field evolution\n",
    "from PARC.visualization import visualization_navier_stokes as viz\n",
    "viz.plot_field_evolution(y_pred, gt, case_id = 4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot MSE, PDE residual graph\n",
    "dt = 2.0/38            # Temporal resolution\n",
    "dx = 1.0/128           # Spatial resolution\n",
    "from PARC import metrics\n",
    "loss_func = metrics.NSPdeLoss(dt, dx)\n",
    "\n",
    "def rmse(pred,gt):\n",
    "    return np.sqrt(np.mean((pred-gt)**2))\n",
    "\n",
    "def get_loss_pinn(gt,pred):\n",
    "    rr = 0.125 + 1.0/128\n",
    "    #rr = 0.0\n",
    "    x, y = np.linspace(0.0, 1.0, 128), np.linspace(0.0, 2.0, 256)\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "    mask = (((xx-0.5)**2+(yy-0.5)**2)<=rr*rr)\n",
    "    # print(mask)\n",
    "\n",
    "    dt = 2.0/38            # Temporal resolution\n",
    "    dx = 1.0/128           # Spatial resolution\n",
    "    # loss_func = loss_generator(dt, dx)\n",
    "    mse_pinn = []\n",
    "    pde_pinn = []\n",
    "    div_pinn = []\n",
    "    # print(pinn.shape)\n",
    "    for i, Re in enumerate(Re_list):\n",
    "        # Ground truth\n",
    "        i_gt = gt[i,:,:,:].reshape(128, 256, 39, 3).transpose(2, 0, 1, 3)[:38, :, :, :]\n",
    "        u_gt = np.sqrt(i_gt[:,:,:,0:1]**2+i_gt[:,:,:,1:2]**2)\n",
    "\n",
    "        # # PhyCRNet\n",
    "        i_pinn = pred[i,:,:,:].reshape(128, 256, 38, 3).transpose(2, 0, 1, 3)[:38, :, :, :]\n",
    "        # i_pinn[:,:,:,2:] /= (4.0*Re)\n",
    "        u_pred = np.sqrt(i_pinn[:,:,:,0:1]**2+i_pinn[:,:,:,1:2]**2)\n",
    "        # np.load(\"data/phycrnet/Re_%i.npy\" % (Re))\n",
    "\n",
    "        # MSE loss\n",
    "        mse_pinn_Re = rmse(u_pred,u_gt)\n",
    "        mse_pinn.append(mse_pinn_Re)\n",
    "        # PDE loss\n",
    "        i_pinn = torch.FloatTensor(i_pinn.transpose(0, 3, 2, 1))\n",
    "        loss_func.set_data(i_pinn)\n",
    "        fu_pinn, fv_pinn, fp_pinn = loss_func.ComputePdeLoss(4.0 * Re, 1.0/(4.0 * Re), mask)\n",
    "        fu_pinn = fu_pinn.numpy()\n",
    "        fv_pinn = fv_pinn.numpy()\n",
    "        fp_pinn = fp_pinn.numpy()\n",
    "        pde_pinn.append(np.mean(np.sqrt(fu_pinn * fu_pinn + fv_pinn * fv_pinn)))\n",
    "        div_pinn.append(np.mean(fp_pinn))\n",
    "\n",
    "    mse_pinn = np.array(mse_pinn)\n",
    "    pde_pinn = np.array(pde_pinn)\n",
    "    div_pinn = np.array(div_pinn)\n",
    "    return mse_pinn, pde_pinn, div_pinn\n",
    "\n",
    "mse_whole = []\n",
    "pde_whole = []\n",
    "div_whole = []\n",
    "for i in range(6):\n",
    "    i_mse_pinn, i_pde_pinn, i_div_pinn = get_loss_pinn(gt, y_pred[i])\n",
    "    mse_whole.append(i_mse_pinn)\n",
    "    pde_whole.append(i_pde_pinn)\n",
    "    div_whole.append(i_div_pinn)\n",
    "    print(i)\n",
    "\n",
    "i_mse_pinn, i_pde_pinn, i_div_pinn = get_loss_pinn(gt, gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.pde_mse_div_Re(mse_whole,pde_whole,div_whole,Re_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow 2.13.0",
   "language": "python",
   "name": "tensorflow-2.13.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
